---
title: 'Pokemon: Who is Legendary? (Quiz)'
author: "Meng-Ni Ho"
date: 'Analyses completed: `r format(Sys.Date(), "%Y-%m-%d")`'
output: 
    html_document:
      toc: true
      toc_depth: 3 # up tp four depths of heading
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Description
In this project, we will be using kNN to predict if the pokemon is legenary or not, using features such as attack, defense abilities, or speed, experience growth profile..etc. The dataset contains 801 observations, 40 features, 1 outcome variables (`is_legendary`). Before training the data with Machine Learning, we also imputed missing data with random forest, and use lasso regression to perform variable selections.

## Load data and library
```{r load, message=FALSE, warning=FALSE}
tdata = read.csv("pokemon.csv", header = T, na.strings = c(" ",''))
library(tidyverse, quietly = T)
library(glmnet, quietly = T)
library(missForest, quietly = T)
library(e1071, quietly = T) 
library(C50, quietly = T)
library(caret, quietly = T)
library(knitr, quietly = T)
library(class, quietly = T)
set.seed(100)
str(tdata)
# remove text formating features: abilities, classfication, japanese_name, name
tdata$capture_rate = as.numeric(tdata$capture_rate)
sdata = tdata %>% select(-c(abilities, classfication, japanese_name, name, pokedex_number)) 
```

## Question 1: Data Cleaning & Data Exploration
### 1. Missing data imputation with Random Forest
In this step, we used random forest to perform missing data imputation. First, we filtered out complete cases in the dataset (N = 339), and generated 10% missingness to perform two types of imputation: (1) set trees to 100, (2) set trees to 200. After, we compared the NRMSE (normalized mean squared error) and PFS (proportion of falsely classified) for both two models, and we found that both models showed similar error. Therefore, we decide to set trees to 200 and perform random forest imputation for the whole data (N = 801).   
1. NRMSE (normalized mean squared error): represent error derived from imputing continuous values.     
2. PFC (proportion of falsely classified):  represent error derived from imputing categorical values.     
```{r impute, warning=FALSE, message=FALSE}
# step 1. use complete cases then generate 10% missing values
x.complete = na.omit(sdata[,-37])
x.mis = prodNA(x.complete, noNA = 0.1)

# step 2-1. impute missing values, ntree = 100
x.imp = missForest(x.mis) # default ntree = 100

# step 2-2: impute missing values, ntree = 200
x.imp2 = missForest(x.mis, ntree = 200) 

# step 3: check imputation error
x.imp$OOBerror  
x.imp2$OOBerror  

# step 4: comparing actual data accuracy
x.err = mixError(x.imp$ximp, x.mis, x.complete)
x.err2 = mixError(x.imp2$ximp, x.mis, x.complete)

err = data.frame(model = c("ntree=100", "ntree=200"), NRMSE = c(x.err[1], x.err2[1]), 
                 PFC = c(x.err[2], x.err2[2]))


# step 5: start imputate all the data using the above `missForest` setting
model.imp = missForest(sdata[,-37], ntree = 200)

# step 6: check imputation error
# NRMSE: 0.0000754659, PFC: 0.0563549161
err_model = data.frame(model = "ntree=200", 
                       NRMSE = format(as.numeric(model.imp$OOBerror[1]), digits = 4), 
                       PFC = format(as.numeric(model.imp$OOBerror[2]), digits = 4))

# step 7: extract imputed data
new.x = model.imp$ximp

```

#### Results after random forest imputation:
(1). NRMSE and PFC for random forest imputation: complete cases with 10% missingness   
```{r}
kable(err)
```


(2). NRMSE and PFC for random forest imputation: whole data
```{r}
kable(err_model)
```


### 2. Convert imputed data to design matrix
In the imputed data, we can see that there are two features that are categories (`type 1`, `type 2`), we used `model,matrix()` to convert imputed data to design matrix
```{r dummy}
# 2. convert to dummy variables
dummy.x = model.matrix( ~ ., model.frame( is_legendary~ ., data = new.x, na.action=na.pass))
dummy.x = dummy.x[,-1] # remove intercept
# colnames(dummy.x)
# dummy.x includes is_legendary (in the first column)
```

### 3. Variable Selection: Lasso Regression
We have total 40 features, after removing features that are in text format (`abilities`, `classfication`, `japanese_name`, `name`), we reached to 37 variables. By fitting lasso regression, we can do variable selections by removing the variables that show coefficient = 0.
```{r lasso}
lasso = cv.glmnet(x = dummy.x[,-1], y = dummy.x[,"is_legendary"], type.measure = 'mse', nfolds = 5, alpha = 1)
coef = coef(lasso, s = 'lambda.min',exact=TRUE)
idx = which(coef!=0)
variables = row.names(coef)[idx]
```

#### These are the varaibles selected after lasso regression:
```{r}
print(variables[-1])
```

### 4. Data Visualization
```{r}
# subset variable selected from lasso
dummy.fit.x = as.data.frame(dummy.x[, c("is_legendary",variables[-1])])
# create new data
new.data = as.data.frame(dummy.fit.x)
# plot some barplot
ggplot(new.data, aes(x = as.factor(is_legendary), y= base_egg_steps, group = as.factor(is_legendary), fill = as.factor(is_legendary))) +
  labs(fill = "is legendary") + 
  scale_y_continuous("Base egg steps") +
  scale_x_discrete("Is legendary") + 
  geom_boxplot()

ggplot(new.data, aes(x = as.factor(is_legendary), y= sp_defense, group = as.factor(is_legendary), fill = as.factor(is_legendary))) +
  labs(fill = "is legendary") + 
  scale_y_continuous("sp defense") +
  scale_x_discrete("Is legendary") + 
  geom_boxplot()

ggplot(new.data, aes(x = as.factor(is_legendary), y= sp_attack, group = as.factor(is_legendary), fill = as.factor(is_legendary))) +
  labs(fill = "is legendary") + 
  scale_y_continuous("sp attack") +
  scale_x_discrete("Is legendary") + 
  geom_boxplot()

ggplot(new.data, aes(x = as.factor(is_legendary), y= speed, group = as.factor(is_legendary), fill = as.factor(is_legendary))) +
  labs(fill = "is legendary") + 
  scale_y_continuous("Speed") +
  scale_x_discrete("Is legendary") + 
  geom_boxplot()
```


## Question 2: Split training and testing data
We split the training and testing set by 7:3 ratio.
```{r}
# split train/test
train_ind = sample(seq_len(nrow(new.data)), size = as.integer(dim(new.data)[1]*0.7))
train = new.data[train_ind,] # N = 560
test = new.data[-train_ind,] # N = 241

```


## Question 3: Model training with KNN

### 1. Find the best k
Now, the data are good to go for training, we trained the model using KNN and test k from 1 to 25. 
```{r}
# scale data with z-score standardization
train.scale = scale(train[, -1])
train.label = train[,"is_legendary"]
#train.scale = as.data.frame(cbind(train.scale, train[,"is_legendary"]))
#colnames(train.scale)[length(colnames(train.scale))] = "is_legendary"
test.scale = scale(test[, -1])
test.label = test[,"is_legendary"]
#test.scale = as.data.frame(cbind(test.scale, test[,"is_legendary"]))
#colnames(test.scale)[length(colnames(test.scale))] = "is_legendary"

# find the best k-value
accuracy.rate = NULL
for(i in 1:25){
    knn_pred_find = knn(train = train.scale, test = test.scale, cl = train.label, k=i)
    accuracy.rate[i] = 1-(mean(test.label != knn_pred_find))
}
print(accuracy.rate)

# find the k with highest accuracy
best_k = which(accuracy.rate == max(accuracy.rate))
final_k = min(best_k)
print(best_k)

```

When k = `r best_k` reach the highest accuracy of `r round(unique(accuracy.rate[best_k])*100, digits=2)`. We choose the smallest k (k = `r final_k`) and train the data again.

### 2. Build the model with best k: 
```{r}
# use the best k to train
knn_pred <- knn(train = train.scale, test = test.scale, cl = train.label, k= final_k)

# confusion matrix
cm.knn <- confusionMatrix(knn_pred, as.factor(test.label))
accuracy.knn <- round(as.numeric(cm.knn$overall[1])*100, digits = 2)
kappa.knn <- round(as.numeric(cm.knn$overall[2]), digits = 2)
pred.table.knn <- cm.knn$table
print(pred.table.knn)
```

Results: Accuracy of using kNN with k = `r final_k` shows `r accuracy.knn`%. Kappa statistics shows `r kappa.knn`.


## Question 4

Assume that you are consulting the Pokemon Manufacturing Company. They have realized that they had a rogue employee who changed designation on a few legendary Pokemons and labeled them as ordinary (non-legendary). The company has asked you for assistance in finding out the extent of this problem and figuring out a process for how to identify Pokemons that have been mislabeled. What will be your advice? This question is open ended by design - make any assumptions that you need to make and state them. Do any data exploration or model building that you need to do and explain them.    
      
    
*Answer*: 
First we can cleaned and do data imputation for the data provided from the manufacturer. Then, we can propose our kNN model to the manufacturers and test their data with the proposed kNN. With the predicted label we get from the testing data, we can compare them to the existed label and identify which labels are being misclassified. Since our model's accuracy is `r accuracy.knn`% and kappa shows `r kappa.knn`, our model is highly reliable and can lower the errors in predicting a false types of legendary. And therefore, our model can help identify a false classified label of which pokemon is legendary.









